apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: monitoring
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
      evaluation_interval: 15s

    # Load rules once and periodically evaluate them
    rule_files:
      - /etc/prometheus/rules/*.yml

    scrape_configs:
      - job_name: 'kubernetes-apiservers'
        kubernetes_sd_configs:
          - role: endpoints
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

      - job_name: 'kubernetes-nodes'
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        kubernetes_sd_configs:
          - role: node
        relabel_configs:
          - action: labelmap
            regex: __meta_kubernetes_node_label_(.+)

      # Required job for Grafana dashboard #315 - Kubernetes cluster monitoring
      - job_name: 'kubernetes-nodes-cadvisor'
        scrape_interval: 10s
        scrape_timeout: 10s
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        kubernetes_sd_configs:
          - role: node
        relabel_configs:
          - action: labelmap
            regex: __meta_kubernetes_node_label_(.+)
          - target_label: __address__
            replacement: kubernetes.default.svc:443
          - source_labels: [__meta_kubernetes_node_name]
            regex: (.+)
            target_label: __metrics_path__
            replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor
        metric_relabel_configs:
          - action: replace
            source_labels: [id]
            regex: '^/machine\.slice/machine-rkt\\x2d([^\\]+)\\.+/([^/]+)\.service$'
            target_label: rkt_container_name
            replacement: '${2}-${1}'
          - action: replace
            source_labels: [id]
            regex: '^/system\.slice/(.+)\.service$'
            target_label: systemd_service_name
            replacement: '${1}'

      - job_name: 'kubernetes-pods'
        kubernetes_sd_configs:
          - role: pod
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
            action: keep
            regex: true
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
            action: replace
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
            target_label: __address__
          - action: labelmap
            regex: __meta_kubernetes_pod_label_(.+)
          - source_labels: [__meta_kubernetes_namespace]
            action: replace
            target_label: kubernetes_namespace
          - source_labels: [__meta_kubernetes_pod_name]
            action: replace
            target_label: kubernetes_pod_name
          - source_labels: [__meta_kubernetes_pod_label_app_kubernetes_io_name]
            action: replace
            target_label: service
      
      # Scrape Loki metrics
      - job_name: 'loki'
        static_configs:
          - targets: ['loki:3100']

      # Scrape kube-state-metrics
      - job_name: 'kube-state-metrics'
        kubernetes_sd_configs:
          - role: endpoints
            namespaces:
              names:
                - monitoring
        relabel_configs:
          - source_labels: [__meta_kubernetes_service_name]
            action: keep
            regex: kube-state-metrics
          - source_labels: [__meta_kubernetes_endpoint_port_name]
            action: keep
            regex: http-metrics

      # Scrape ingress-nginx metrics (controller)
      - job_name: 'ingress-nginx'
        kubernetes_sd_configs:
          - role: endpoints
            namespaces:
              names:
                - ingress-nginx
        relabel_configs:
          - source_labels: [__meta_kubernetes_service_label_app_kubernetes_io_name]
            action: keep
            regex: ingress-nginx
          - source_labels: [__meta_kubernetes_service_name]
            action: keep
            regex: ingress-nginx-controller-metrics|ingress-nginx-controller
          - source_labels: [__meta_kubernetes_endpoint_port_name]
            action: keep
            regex: metrics|http-metrics

      # Fallback: scrape ingress-nginx controller pods directly on port 10254
      - job_name: 'ingress-nginx-pods'
        kubernetes_sd_configs:
          - role: pod
            namespaces:
              names:
                - ingress-nginx
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_label_app_kubernetes_io_name]
            action: keep
            regex: ingress-nginx
          - source_labels: [__meta_kubernetes_pod_label_app_kubernetes_io_component]
            action: keep
            regex: controller
          - source_labels: [__meta_kubernetes_pod_ip]
            action: replace
            regex: (.+)
            target_label: __address__
            replacement: ${1}:10254

      # Scrape Echo Application metrics
      - job_name: 'echo-application'
        kubernetes_sd_configs:
          - role: pod
            namespaces:
              names:
                - echo-dev
                - echo-prod
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_label_app]
            regex: echo
            action: keep
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
            action: keep
            regex: true
          - source_labels: [__meta_kubernetes_namespace]
            target_label: namespace
          - source_labels: [__meta_kubernetes_pod_name]
            target_label: pod
          - source_labels: [__meta_kubernetes_pod_label_component]
            target_label: component

      # Scrape Promtail metrics
      - job_name: 'promtail'
        kubernetes_sd_configs:
          - role: pod
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_label_app]
            regex: promtail
            action: keep
          - source_labels: [__meta_kubernetes_pod_container_port_name]
            regex: http-metrics
            action: keep
          - source_labels: [__meta_kubernetes_namespace]
            target_label: namespace
          - source_labels: [__meta_kubernetes_pod_name]
            target_label: pod
          - source_labels: [__address__]
            target_label: __address__
            regex: (.+)
            replacement: $1
            
      # Scrape node-exporter metrics
      - job_name: 'node-exporter'
        kubernetes_sd_configs:
          - role: endpoints
            namespaces:
              names:
                - monitoring
        relabel_configs:
          - source_labels: [__meta_kubernetes_service_name]
            action: keep
            regex: node-exporter
          - source_labels: [__meta_kubernetes_endpoint_port_name]
            action: keep
            regex: metrics

  node-alerts.yml: |
    groups:
    - name: node
      rules:
      - alert: HighNodeCPU
        expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High CPU usage on {{ "{{" }} $labels.instance {{ "}}" }}"
          description: "CPU usage is above 80% on {{ "{{" }} $labels.instance {{ "}}" }} for more than 5 minutes"

      - alert: HighNodeMemory
        expr: (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes * 100 > 85
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage on {{ "{{" }} $labels.instance {{ "}}" }}"
          description: "Memory usage is above 85% on {{ "{{" }} $labels.instance {{ "}}" }} for more than 5 minutes"

      - alert: NodeDiskFillingSoon
        expr: predict_linear(node_filesystem_free_bytes{mountpoint="/"}[6h], 24 * 3600) < 0
        for: 1h
        labels:
          severity: warning
        annotations:
          summary: "Disk filling up on {{ "{{" }} $labels.instance {{ "}}" }}"
          description: "Disk is predicted to fill up within 24 hours on {{ "{{" }} $labels.instance {{ "}}" }}"

  app-alerts.yml: |
    groups:
    - name: application
      rules:
      - alert: IngressHighErrorRate
        expr: (sum by (svc)( label_replace(rate({__name__=~"nginx_ingress_controller_requests(_total)?",status=~"5..",namespace="echo-prod"}[5m]), "svc", "$1", "exported_service", "(.*)") or label_replace(rate({__name__=~"nginx_ingress_controller_requests(_total)?",status=~"5..",namespace="echo-prod"}[5m]), "svc", "$1", "service", "(.*)") )) / (sum by (svc)( label_replace(rate({__name__=~"nginx_ingress_controller_requests(_total)?",namespace="echo-prod"}[5m]), "svc", "$1", "exported_service", "(.*)") or label_replace(rate({__name__=~"nginx_ingress_controller_requests(_total)?",namespace="echo-prod"}[5m]), "svc", "$1", "service", "(.*)") )) > 0.05
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High error rate via ingress on {{ "{{" }} $labels.svc {{ "}}" }}"
          description: "5xx ratio > 5% over 10m: {{ "{{" }} $value {{ "}}" }}"

      - alert: IngressCriticalErrorRate
        expr: (sum by (svc)( label_replace(rate({__name__=~"nginx_ingress_controller_requests(_total)?",status=~"5..",namespace="echo-prod"}[5m]), "svc", "$1", "exported_service", "(.*)") or label_replace(rate({__name__=~"nginx_ingress_controller_requests(_total)?",status=~"5..",namespace="echo-prod"}[5m]), "svc", "$1", "service", "(.*)") )) / (sum by (svc)( label_replace(rate({__name__=~"nginx_ingress_controller_requests(_total)?",namespace="echo-prod"}[5m]), "svc", "$1", "exported_service", "(.*)") or label_replace(rate({__name__=~"nginx_ingress_controller_requests(_total)?",namespace="echo-prod"}[5m]), "svc", "$1", "service", "(.*)") )) > 0.20
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Critical error rate via ingress on {{ "{{" }} $labels.svc {{ "}}" }}"
          description: "5xx ratio > 20% over 5m: {{ "{{" }} $value {{ "}}" }}"

      - alert: IngressHighLatencyP95
        expr: histogram_quantile(0.95, sum by (svc, le) ( label_replace(rate(nginx_ingress_controller_request_duration_seconds_bucket{namespace="echo-prod"}[5m]), "svc", "$1", "exported_service", "(.*)") or label_replace(rate(nginx_ingress_controller_request_duration_seconds_bucket{namespace="echo-prod"}[5m]), "svc", "$1", "service", "(.*)") )) > 0.5
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High p95 latency via ingress on {{ "{{" }} $labels.svc {{ "}}" }}"
          description: "> 0.5s over 10m: {{ "{{" }} $value {{ "}}" }}"

      - alert: IngressCriticalLatencyP95
        expr: histogram_quantile(0.95, sum by (svc, le) ( label_replace(rate(nginx_ingress_controller_request_duration_seconds_bucket{namespace="echo-prod"}[5m]), "svc", "$1", "exported_service", "(.*)") or label_replace(rate(nginx_ingress_controller_request_duration_seconds_bucket{namespace="echo-prod"}[5m]), "svc", "$1", "service", "(.*)") )) > 1.5
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Critical p95 latency via ingress on {{ "{{" }} $labels.svc {{ "}}" }}"
          description: "> 1.5s over 5m: {{ "{{" }} $value {{ "}}" }}"

      - alert: CrashLoopBackOff
        expr: sum by (namespace, pod, container) (kube_pod_container_status_waiting_reason{reason="CrashLoopBackOff", namespace="echo-prod"}) > 0
        for: 10m
        labels:
          severity: critical
        annotations:
          summary: "CrashLoopBackOff: {{ "{{" }} $labels.namespace {{ "}}" }}/{{ "{{" }} $labels.pod {{ "}}" }} ({{ "{{" }} $labels.container {{ "}}" }})"

      - alert: PodRestartSpike
        expr: sum by (namespace, pod) (increase(kube_pod_container_status_restarts_total{namespace="echo-prod"}[5m])) > 3
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Restart spike: {{ "{{" }} $labels.namespace {{ "}}" }}/{{ "{{" }} $labels.pod {{ "}}" }}"

      - alert: OOMKilled
        expr: sum by (namespace, pod, container) (increase(kube_pod_container_status_terminated_reason{reason="OOMKilled", namespace="echo-prod"}[5m])) > 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "OOMKilled: {{ "{{" }} $labels.namespace {{ "}}" }}/{{ "{{" }} $labels.pod {{ "}}" }} ({{ "{{" }} $labels.container {{ "}}" }})"

      - alert: HighContainerCPUUtilization
        expr: ( sum by (pod, namespace)(rate(container_cpu_usage_seconds_total{namespace="echo-prod", image!="", container!=""}[5m])) ) / clamp_min(sum by (pod, namespace)(kube_pod_container_resource_requests{namespace="echo-prod", resource="cpu", unit="core", container!=""}), 0.1) > 0.9
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High CPU utilization vs requests: {{ "{{" }} $labels.namespace {{ "}}" }}/{{ "{{" }} $labels.pod {{ "}}" }}"

      - alert: HighContainerMemoryUtilization
        expr: ( sum by (pod, namespace)(container_memory_working_set_bytes{namespace="echo-prod", image!="", container!=""}) ) / clamp_min(sum by (pod, namespace)(kube_pod_container_resource_limits{namespace="echo-prod", resource="memory", unit="byte", container!=""}), 1) > 0.9
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High Memory utilization vs limits: {{ "{{" }} $labels.namespace {{ "}}" }}/{{ "{{" }} $labels.pod {{ "}}" }}"